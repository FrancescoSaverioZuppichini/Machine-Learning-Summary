\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{biblatex}
\bibliography{main} 
\addbibresource{main.bib}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{caption}
\usepackage{float}
\usepackage{todonotes}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
\usepackage{sidecap}

\title{Machine Learning Review}
\author{Francesco Saverio Zuppichini}
\begin{document}
\maketitle
The aim of this report is to summarise the topics threaded in my Machine Learning class at USI.
\section{History of Machine Learning}
\todo[inline]{Write something about the history of ML}

\section{Gradient Descend}
The gradient descend is an iterative optimisation algorithm that follows the direction of the negative gradient in order to minimised an objective function. It can be effectively used as Learning Algorithm because it reduces the error function, Equation \ref{eq: MSE_perceptron}, and adjusts the weights properly. Equation \ref{eq: GD} shows the generic update rule.
\begin{equation}
\label{eq: gradient_descent}
	w_{k + 1} = w_k - \eta \nabla E(w_k)
\end{equation}
Where $\eta$ is the step size, also called \textbf{learning rate} in Machine Learning. This parameter influences the behaviour of gradient descent, a small number can lead to local minimum, while a bigger learning rate could "over-shoot" and decreasing the converge ration. Later in this project you will see how a wrong $\eta$ can strongly change the output of a Neural Network.

For this reasons, numerous improvements have been proposed to avoid local minima and increase its convergence ration, some of them are: Conjugate Gradient and Momentum.
\section{Perceptron}
\section{Definition}
The \textbf{Perceptron} is binary  \textbf{linear classifier} algorithm used in \textbf{supervised learning}. It can be seen as the most basic form of Neural Network. Equation \ref{eq: perceptron} defines the its output.
\begin{equation}
f(x) \left \{ 
 \begin{tabular}{lcc}
  1 \quad \text{if} w \cdot x + b \ge  0 & & \\ 
  0 \quad \text{otherwise} &  & 
  \end{tabular}	
  \label{eq: perceptron}
\end{equation}
Given a training set $D = \{ (x_1,t_1}, ... , (x_n,t_i) \}$, $x_i \in X$ and $t_i \in Y$ denotes the input vector and the target vector respectively. We express $y = f(x)$ as the output of the algorithm, $w$ the weight and $b$ the bias. At each iteration the error is calculated using the Mean Square Error, defined in equation \ref{eq: MSE_perceptron}
\begin{equation}
	E(w) = \frac{1}{N}\sum_{i = 1}^N(\underbrace{y(x_i,w_i)}_{\text{predicted}} - \underbrace{t_i}_{\text{actual}})^2)
	\label{eq: MSE_perceptron}
\end{equation}
The algorithm uses stochastic \textbf{Gradient Descent} in order to update the weight at each iteration using the formula defined in Equation \ref{eq: gradient_descent}.
 
\begin{equation}
	\frac{\partial E}{\partial w_k} = y - t
\end{equation}
\todo[inline]{Decide if use \nabla or partial or booth}
\section{Neural Network}
A \textbf{Neural Network} is a universal function approximation. It is a nested composite functions like $f(g((h...)))$. In its simplest representation, an FeedForward Neural Network, it is composed by a \textbf{input layer}, an \textbf{hidden layer} and an \textbf{output layer}. The size of the hidden layer is usually refers as the \textbf{deepth} of the network.
\subsubsection{Forward pass}
In order to get the prediction out of our network we need to calculate the compute the activation at each layer $l$.
Equation \ref{eq:forward_pass} shows the activation $a$ of layer $l$ for the $j$-th neuron on that layer.

\begin{equation}
\label{eq:forward_pass}
a^l_j = \sigma(\sum_k w^l_{jk}a^{l-1}_k + b^l_j)
\end{equation}

Where $w^l_{jk}$ is the connection from neuron $k$ in the $l-1$ layer to $j$, $a^{l-1}$ is the activation of the previous layer and $b^l_j$ is the bias of $j$-th neuron in the $l$ layer. With this in mind, we can rewrite \ref{eq:forward_pass_vectorized} in a efficient vectorised form
\begin{equation}
\label{eq:forward_pass_vectorized}
a^l = \sigma(W^la^{l-1} + b^l)
\end{equation}

\subsubsection{Delta rules}
In a Neural Network the weights are iteratively changed in order to decrease the cost function, called $E$. We want to find out how much they should be updated, in order to do so we need the output error at each layer. Equation \ref{eq:deltaRule_1} defines $\delta^l_j$ as the output error of neuron $j$ in layer $l$
\begin{equation}
	\delta^l_j = \frac{\partial E}{\partial z^l_j}
	\label{eq: deltaRule_1}
\end{equation}
Strictly speaking, $\delta^l_j$, is how much the error function changes by changing the weighted input on that layer. Applying the chain rule, Equation \ref{eq: deltaRule_1} becomes:
\begin{equation}
\delta^l_j = \frac{\partial E}{\partial a^l_j} \frac{\partial a^l_j}{\partial z^l_j}
\label{eq:deltaRule_2}
\end{equation}
By knowing that $a^l_j = \sigma(z^l_j)$, Equation \ref{eq:deltaRule_2} can be expressed as
\begin{equation}
\delta^l_j = \frac{\partial E}{\partial a^l_j} \sigma'(z^l_j)
\label{eq:deltaRule}	
\end{equation}
Also, delta at layer $l$ can be expressed by using the next $l+1$-th delta. Equation \ref{eq: delta_on_delta} shows the new rule.
\begin{equation}
\delta^l = (W^{l+1}\delta^{l+1}) * a^l
\label{eq: delta_on_delta}
\end{equation}
\subsubsection{Back Propagation}
The \textbf{Back Propagation} algorithm defines an efficient and interactive method to calculate the gradient at each layer. We want to compute $\frac{\partial E}{\partial w^l_{jk}}$. We can applying the delta rule: 
\begin{equation}
\frac{\partial E}{\partial w^l_{jk}} = \frac{\partial E}{\partial z^l_j}\frac{\partial z^l_j}{\partial w^l_{jk}} =	
\frac{\partial E}{\partial a^l_j}\frac{\partial a^l_j}{\partial z^l_{j}}
\end{equation}
After some calculation, Equation \ref{eq: back_propagation} shows how to calculate the gradient for the weight $w$ of the $l$-th layer for the $j$-th neuron.
\begin{equation}
\frac{\partial E}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j
\label{eq: back_propagation}
\end{equation}  
While $\delta^l$ can be calculated used the $l+1$-th layer's information, this is why is called \emph{back} propagation since we used the last layer information in order to, each time, find out the next layer's delta until we reach the input layer.
\section{Convolutional Neural Network}
\section{Recurrent Neural Network}
\subsection{Definition}
A Recurrent Neural Networks can remember past decision by taking as input not only the current input but also the last time state. For this reason it is said that a RNN has \textbf{memory}. Figure \ref{fig: RNN} shows a classic representation. Usually, a RNN is represented unfolded to highlight the time dependencies. Due to its ability to remember it mostly used in text and speech recognition.
\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{images/rnn}	
\caption{Fold and Unfold representation of a RNN}
\label{fig: RNN}
\end{figure}

Very similar to a feedforward network, Equation \ref{eq: output_rnn} shows the weighted output at layer $j$. The first term on the right-hand is just the feedforward's weighted output, while the second term is the time-dependent term. Matrix $\omega$ is a hidden-state-to-hidden-state matrix. Basically, we are adding previous informations to our new state at time $t$.

\begin{equation}
z[t]^l = (W^la[t]^{l -1}+ b^l_j) + (\omega^l a[t-1]^{l-1})
\label{eq: output_rnn}	
\end{equation}
Equation \ref{eq: activation_rnn} shows the activation of layer $j$ at time $t$.
\begin{equation}
a[t]^l	= \sigma(z[t]^l)
\label{eq: activation_rnn}
\end{equation}
While Equation \ref{eq: rnn_weigt_update} shows the updating rule for the weight $w^l$
\begin{equation}
\begin{matrix}
\frac{\partial E}{\partial w^[T]l_{jk}} =\sum^T_{t=0} \frac{\partial E}{\partial z[t]^l_j}	\\ 
\\
\frac{\partial E}{\partial w^[T]l_{jk}} = \sum^T_{t=0} a[t]^{l-1}_k \delta[t]^l_j	
\end{matrix}
\label{eq: rnn_weigt_update}
\end{equation}

\subsection{Vanishing Gradient Problem}
Since the network layers and time steps are related to each other through multiplication, the gradient becomes smaller and smaller at each $t$. Figure \ref{fig: sigmoid_over_time} shows the effect of applying $sigmoid$ function over time. The data is flatted more and more at each step and therefore the slope will $\rightarrow 0$.
\begin{figure}[H]
\includegraphics[scale=0.5]{images/sigmoid_gradient_vanish}
\includegraphics[scale=0.5]{images/sigmoid_gradient_vanish_derivative}
\caption{Sigmoid and its derivative over time}
\label{fig: sigmoid_over_time}	
\end{figure}
Since gradient expresses the change in all weights with respected to the error, without nothing it we cannot adjust properly the network.

\section{Long Short Term Memory}
\subsection{Definition}
The Long Short Term Memory networks, or just \textbf{LSTM}, are a special type of RNN capable of learning long-term
dependencies. They were introduced by Juergen Schmidhuber in order to solve the vanish gradient problem. They are composed by LSTM cell, 
Figure \ref{fig: unrolled_LSTM} shows a unrolled representation.
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./images/LSTM3-chain.png}	
\caption{An unrolled LSTM}
\label{fig: unrolled_LSTM}
\end{figure}
Each cell is composed by 3 gate: \textbf{forget} gate ($f_t$), \textbf{input} gate $i_t$ and \textbf{output} gate ($o_t$). It takes as 
input the previous output $h_{t - 1}$ and the old cell state $C_{t-1}$, it outputs the next prediction and state, $h_{t}$ and $C_t$. The cell
computes four basic operations:  
% \begin{enumerate}
	% \item
\begin{enumerate}
	\item Forget Gate \\ 
		\includegraphics[scale=0.5]{./images/LSTM3-focus-f.png}
\item Input Gate \\
 \includegraphics[scale=0.5]{./images/LSTM3-focus-i.png}
\item Update Cell State \\
		\includegraphics[scale=0.5]{./images/LSTM3-focus-C.png}
\item Output Gate	\\
\includegraphics[scale=0.5]{./images/LSTM3-focus-o.png}
\end{enumerate}
	
% \end{enumerate}
\section{Support Vector Machine}
\section{Deep Learning}
\subsection{Supervised Learning}
In \textbf{supervised} learning there is a fixed training set consists on a set of example patterns and their targets. The objective is to learn a map between the inputs and the targets. 'Supervised' comes from the likeness between a student that tries to answer a question, and a teacher that knows the solution.

Therefore, at each step the algorithm can know how much wrong is prediction was, using a cost function, and it can improve it by applying a learning algorithm such as gradient descent in order to reduce the error.

\subsection{Reinforcement Learning}
In \textbf{reinforcement} learning the \emph{agent} takes \emph{actions} in an \emph{environment} in an to maximise a \emph{reward}. Figure \ref{fig: reinforcement_flow} shows the actions flow.

\begin{figure}[h]
\centering
	\includegraphics[scale=0.5]{images/reinforcement}
	\caption{Reinforcement Learning Flow}
	\label{fig: reinforcement_flow}
\end{figure}
\subsection{Unsupervised Learning}

\subsection{Training Techniques}
\subsubsection{Mini-batch}
\subsection{Regularisation}
\subsubsection{L1 Regularisation}
\subsubsection{Dropout Regularisation}
\subsection{Activations Functions}
\end{document}
